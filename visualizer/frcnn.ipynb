{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.config import opt\n",
    "from data.dataset import TestDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.extraction_point = [15, 22, 29]\n",
    "        self.extractor = load_vgg16_extractor()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features, indices, in_shapes = {}, {}, {}\n",
    "        for i, layer in enumerate(self.extractor):\n",
    "            if i in self.extraction_point:\n",
    "                x = layer(x)\n",
    "                features[i] = x\n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                in_shapes[i] = x.shape[2:]\n",
    "                x, idx = layer(x)\n",
    "                indices[i] = idx\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        \n",
    "        return features, indices, in_shapes\n",
    "\n",
    "def load_vgg16_extractor():\n",
    "    model = models.vgg16()\n",
    "    features = list(model.features)[:-1]\n",
    "    for i in range(len(features)):\n",
    "        if isinstance(features[i], nn.MaxPool2d):\n",
    "            features[i] = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        for p in features[i].parameters():\n",
    "            p.requires_grad = False\n",
    "    return nn.Sequential(*features)\n",
    "\n",
    "class DeconvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeconvNet, self).__init__()\n",
    "        self.extractor = load_vgg16_extractor_reverse()\n",
    "    \n",
    "    def forward(self, features, indices, in_shapes):\n",
    "        remapped = {}\n",
    "        for key, feature in features.items():\n",
    "            for i, layer in reversed(list(enumerate(self.extractor[:key + 1]))):\n",
    "                if isinstance(layer, nn.MaxUnpool2d):\n",
    "                    feature = layer(feature, indices[i], output_size=in_shapes[i])\n",
    "                else:\n",
    "                    feature = layer(feature)\n",
    "            remapped[key] = feature\n",
    "        return remapped\n",
    "\n",
    "def load_vgg16_extractor_reverse():\n",
    "    model = models.vgg16()\n",
    "    features = list(model.features)[:-1]\n",
    "    for i in range(len(features)):\n",
    "        if isinstance(features[i], nn.MaxPool2d):\n",
    "            features[i] = nn.MaxUnpool2d(2, stride=2)\n",
    "        elif isinstance(features[i], nn.Conv2d):\n",
    "            features[i] = nn.ConvTranspose2d(features[i].out_channels,\n",
    "                                            features[i].in_channels,\n",
    "                                            3, 1, padding=1, bias=False)\n",
    "        for p in features[i].parameters():\n",
    "            p.requires_grad = False\n",
    "    return nn.Sequential(*features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_normalize(img):\n",
    "    tensor = img.clone()\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    for t, m, s in zip(tensor[0], mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "def get_most_activated(feature, n_pixel):\n",
    "    dim3_idx = feature.flatten(start_dim=2).argsort(descending=True)\n",
    "    dim3_idx = dim3_idx[0, :, n_pixel:]\n",
    "    dim2_idx = [[i] * dim3_idx.shape[1] for i in range(feature.shape[1])]\n",
    "    feature.flatten(start_dim=2)[0, dim2_idx, dim3_idx] = 0\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load pretrained model & test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load state dictionary for convnet\n",
    "state_dict = torch.load('./frcnn.pth')\n",
    "temp = OrderedDict()\n",
    "for key, item in state_dict.items():\n",
    "    lst = key.split('.')\n",
    "    if 'extractor' in lst:\n",
    "        temp[key] = item\n",
    "        \n",
    "# construct vgg16\n",
    "convnet = ConvNet()\n",
    "convnet.load_state_dict(temp)\n",
    "\n",
    "temp = OrderedDict()\n",
    "for key, item in state_dict.items():\n",
    "    lst = key.split('.')\n",
    "    if 'extractor' in lst and 'weight' in lst:\n",
    "        temp[key] = item\n",
    "\n",
    "deconvnet = DeconvNet()\n",
    "deconvnet.load_state_dict(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = TestDataset(opt)\n",
    "test_loader = DataLoader(test_set, batch_size=1, num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [01:20,  2.77s/it]\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "for i, data in tqdm(enumerate(test_loader)):\n",
    "    if i == 29:\n",
    "        break\n",
    "    \n",
    "    if i not in [0, 6, 16, 21, 28]:\n",
    "        continue\n",
    "    \n",
    "    img, _, _, _, _ = data\n",
    "    outputs = [inverse_normalize(img)]\n",
    "    features, indices, in_shapes = convnet(img)\n",
    "    \n",
    "#     n_pixel = 1000\n",
    "#     # get most activated 500 pixels per feature\n",
    "#     for key in features.keys():\n",
    "#         features[key] = get_most_activated(features[key], n_pixel)\n",
    "    features = deconvnet(features, indices, in_shapes)\n",
    "    # min-max normalization\n",
    "    for feature in features.values():\n",
    "        feature = (feature - feature.min()) / (feature.max() - feature.min())\n",
    "        outputs.append(feature)\n",
    "    # concatenate all features\n",
    "    outputs = torch.cat(outputs)\n",
    "    outputs = torchvision.utils.make_grid(outputs, nrow=2)\n",
    "    writer.add_image(f'frcnn {i + 1}-th feature all pixel', outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
